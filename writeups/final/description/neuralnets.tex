\subsection{Neural Networks}
Neural network based learning (i.e. deep learning) is a type of supervised learning that involves using black-box models which depend on a large number of tunable parameters to classify complicated inputs.
The models are trained by iteratively evaluating their performance on training examples and updating their parameters accordingly, usually through gradient descent.
We call these black-box models `networks' because they are generally conceptualized as a series of composed functions, or `layers', where each layer linearly maps some input vector (or, more generally, tensor) to a possibly differently-sized output vector, and then applies a simple element-wise non-linear `activation function'.
This sequence of highly inter-connected linear functions interspersed with element-wise non-linearities enables the model to identify complex features only using parameters that have relatively simple and computable gradients, ensuring efficient training.

\subsubsection{Fully-connected neural networks}
The traditional neutral network (NN) is a series of fully-connected layers, meaning each input element of a layer has a parameter through which it can linearly affect each output element.
In particular, the function associated with each layer is of the form \textcolor{blue}{\autocite{Nielsen_2015}}
\begin{equation*}
  \begin{aligned}
    \vect{v}_{\text{out}} = f(\vect{v}_{\text{in}}) = \sigma\left(\mathbf{W}\vect{v}_{\text{in}} + \vect{b}\right) ~,
  \end{aligned}
\end{equation*}
where $\mathbf{W}$ is a matrix of `weight' parameters, $\vect{b}$ is a vector of `bias' parameters, and $\sigma$ is the non-linear activiation function.
The derivative of the output vector with respect to the input vector and these parameters can then be simply calculated as
\begin{equation*}
  \begin{aligned}
    \frac{\partial v_{\text{out}, i}}{\partial W_{i, j}} = v_{\text{in}, j} \cdot \sigma'\left(\vect{x}\right)\hspace{3pt}, \hspace{30pt} \frac{\partial v_{\text{out}, i}}{\partial b_{i}} & = \sigma'\left(\vect{x}\right)\hspace{3pt},\text{ and} \hspace{23pt} \frac{\partial v_{\text{out}, i}}{\partial v_{\text{in}, j}} = W_{i, j} \cdot \sigma'\left(\vect{x}\right) ~,
  \end{aligned}
\end{equation*}
where $\vect{x} = \mathbf{W}\vect{v}_{\text{in}} + \vect{b}$ and all unspecified derivatives are zero.
Thus, the output of the entire neural-network model can be simply computed as the composition of each layer function, and the gradient of the loss with respect to the model parameters (to use for parameter updates) can be determined by iteratively applying the chain rule to the above derivatives.

\begin{center}
 \textbf{Implementation:}
\end{center}
The NeuralNetwork implementation within ML Kit is straightforward and exncapsulates everything needed to build, inspect, and run a feed-forward network.
Initially, a NeuralNet holds three parallel vectors: 
\begin{enumerate}
  \item Weight Matrices $Vec<Matrix<f64>>$, each matrix represents the weight between two layers.
  \item Bias Vectors, one per non-input layer. Each is stored in a column sized vector of size $m * 1$ where m is the number of neurons in said layer.
  \item Activation Functions, a vector of activations indicate which activation is to be used upon each layer. Sigmoid, Relu, etc. 
\end{enumerate}

Our NeuralNet construction allows for two ways of instantiation. You can call NeuralNet::new(weights, biases, activations) to instantiate a network of user provided parameters.
Otherwise, for ease of testing, you can call NeuralNet::from\_shape(shape, activations)
to create a zero initialized network and fill with NeuralNet::random\_network(shape, activations).

Once instantiated, our compute\_final\_layer(input) function incorporates the forward pass logic. Starting with a column vector, for each layer 
$l$ we multiply with weight matrix $W_l$ and add bias $b_l$, and apply the element wise activation. The resulting column is then 
carried into the next layer until we return the output vector. We also incorporate functions such as compute\_raw\_layers() and compute\_raw\_and\_full\_layers(). 

We also implemented a paramter\_count() method to tally all weights and biases. Furthermore, shape() reports the complete layer dimensions. 
classify(input) runs a forward pass and returns the neuron with max output. We also incorporated file I/O functionality through the write\_to\_file 
method. The function essentially encodes the number of layers, size, activations, etc.  



\subsubsection{Convolutional neural networks}
Fully-connected neural networks are very general, but they do not take advantage of any potential structure of the input to minimize the number of required parameters or improve efficiency or performance.
Enter, convolutional neural networks (CNNs).
CNNs, primarily designed for image recognization and classification, use the spatial structure of an image to allow for shared weights and biases, reducing the number of required parameters.
In particular, CNNs scan a series of filters across the image, each of which conceptually is intended to identify a certain local feature of the image (e.g. identify straight or curved lines).
This scanning process is specifically done by convolving each filter with the image, hence the `convolutional' in CNN.
As described in \textcolor{blue}{\autocite{Making_faster}}, this convolution step can be implemented efficiently via matrix multiplication by first reshaping the input matrices and filters.
Although we do not give the explicit formulas here (see \textcolor{blue}{\autocite{Solai_2018}}), the derivatives of the output of a convolution layer with respect to the inputs and filters can also be computed via a convolution.

CNNs often also contain pooling layers, which further reduce the dimensionality of the data.
These layers scan a `window' along the input matrix, combining all the elements in each window into a single output element.
This combination process can be done in a number of ways, including taking the maximum, average, or sum of the window elements, which are referred to as max pooling, average pooling, and sum pooling, respectively.
These pooling layers do not have any parameters, and their outputs are simply related to the elements of each window, so the derivatives of this layer are straightforward to compute.

A CNN usually involves a sequence of several of these convolution and pooling layers.
However, once the dimensionality of the input data has been sufficiently reduced, the final classification is usually performed by a fully-connected neural network, which constitutues the last few layers of the CNN.
As with the fully-connected NN, training proceeds by iteratively passing the training inputs through each layer to get the model output, and then moving back through the layers via the chain rule to compute the loss gradient.

\begin{center}
 \textbf{Implementation:}
\end{center}

Following the specifications above, the Convolutional Neural Network implementation within ML Kit implements a sequence of three kinds of layers. 
We define a type enum Layer to consist of the convolutional, pooling, and fully connected layers. 

We implement convolutional layers $ConvLayer$ that store a bank of multi-depth filters with a per filter bias. Furthermore, we included hyperparameters 
such as stride and padding. Pooling Layers implementex max, avg, or sum pooling and include the forward propagation abd backprop functionality in the feedforward 
and backprop methods respectively. We implement fully connected layeres $FullLayer$ as discussed above within the Fully Connected Neural Network section. 

These layers are wrapped in our CNN datastructure $ConvNeuralNet$ which holds the subsequent methods. The compute\_final\_layaer() method sequentially calls the feed\_forward method of each respective layer, returning final activations. The populate\_gradients() method does a forward pass to record activations, computes.
each intial gradient, and backtracks via calling each layers back\_prop function, reshaping when needed. Grad descent step calls each layers update\_params() method. 
sgd\_batch\_step() essentially does a forward and backward pass given the functions above and updates layers accordingly. train\_sgd() loops over user defined epoch and applies 
sgd\_batch\_step(). 


\subsubsection{Stochastic gradient descent}
As referenced above, the training of these neural networks is often powered by stochastic gradient descent (SGD).
SGD is a scheme for updating the paraemters of a network using the gradient of the loss for small sampled batches of training examples.
In particular, to perform SGD, a set of training examples is provided, along with a specification of batch size and number of epochs.
In each epoch, the whole set of training examples is sampled into different smaller sets specified by the batch size.
The network parameters are then updated based on the loss incurred by each batch of training examples, rather than the entire training set.
This greatly reduces the computational cost in each update step, while not changing the performance of the model in expectation.
This is the only type of gradient descent we implemented.





